{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import csv\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import datetime\n",
    "\n",
    "#### dslquery is a wrapper function for the Dimensions DSL API - just get in touch with the dimensions team for a copy###\n",
    "from dimension_api import api_query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Generate Researcher Collaboration Network from Publications   #######\n",
    "#\n",
    "#  extract from the list of publications identified by doi the researchers that have been identified by researcher_id\n",
    "#  for each resaercher, establish their coauthors, number of publications, most recent insitution, country, \n",
    "#  and dominant FOR code\n",
    "#\n",
    "start = datetime.datetime.now()\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def publicationsfromdoi(dois,limit=1000,skip=0):\n",
    "    searchstring = \"\"\"\n",
    "    search publications\n",
    "       where\n",
    "          doi in [{}]        \n",
    "          and year in [2012:2017]\n",
    "          and type = \"article\"\n",
    "    return publications[year+author_affiliations+FOR] limit {} skip {}\n",
    "    \"\"\".format(\",\".join([ '\"{}\"'.format(d) for d in dois]),limit,skip)\n",
    "    return searchstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eePublicationsfromlist():\n",
    "    pubs=[]\n",
    "    dois = []\n",
    "    with open('Authors_DOIs.txt', 'rt') as csvfile:\n",
    "        doireader = csv.reader(csvfile, delimiter='\\t', quotechar='|')\n",
    "        for row in doireader:\n",
    "            dois.append(row[0])\n",
    "                \n",
    "    skip = 0\n",
    "    \n",
    "    idchunks = [dois[x:x + 250] for x in range(0, len(dois), 250)]\n",
    "    for ids in idchunks:\n",
    "        results = api_query(publicationsfromdoi(dois=ids,limit=1000,skip=skip)).get('publications',[])\n",
    "        pubs += results\n",
    "        print(len(pubs))\n",
    "        \n",
    "    return pubs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get all the publications\n",
    "allpublications = eePublicationsfromlist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Edges\n",
    "\n",
    "### Return a list of Researchers in each publication\n",
    "researchers = [ [r['researcher_id'] for r in a if 'researcher_id' in r.keys()]               \n",
    "                for p in allpublications\n",
    "                for a in p.get('author_affiliations',[])\n",
    "               ]\n",
    "\n",
    "#### Create Edges for each publications \n",
    "\n",
    "cartesian_resaerchers = [list(itertools.product(rs,rs)) for rs in researchers]\n",
    "print(cartesian_resaerchers[0]\n",
    "     )\n",
    "\n",
    "#### 'flatten' the list so that we can count the edges\n",
    "flat_list = [edge_instance \n",
    "             for publist in cartesian_resaerchers \n",
    "             for edge_instance in publist\n",
    "               if edge_instance[0] < edge_instance[1]\n",
    "                 ]\n",
    "\n",
    "#Count the edges, and create a list\n",
    "edges = Counter(flat_list)\n",
    "[ (e,edges[e]) for e in list(edges)]\n",
    "\n",
    "\n",
    "#for affiliations in [p['author_affiliations'] for p in allpublications[1]]:\n",
    "#    print(affiliaitons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Nodes\n",
    "\n",
    "researcher_details = [ [ (r,p.get('year'),p.get('FOR',[])) \n",
    "                 for r in a\n",
    "                 if 'researcher_id' in r.keys()]\n",
    "     for p in allpublications\n",
    "     for a in p.get('author_affiliations',[])\n",
    "]\n",
    "\n",
    "\n",
    "researcher_instance_list = [r_instance \n",
    "             for publist in researcher_details\n",
    "             for r_instance in publist\n",
    "                 ]\n",
    "\n",
    "\n",
    "researcher_size = Counter([ri[0]['researcher_id'] for ri in researcher_instance_list])\n",
    "\n",
    "nodes = {}\n",
    "\n",
    "def updateresearherdef(ri,previousri):\n",
    "    \n",
    "    researchFields, primaryFOR = [], ''\n",
    "    \n",
    "    researchFields = previousri.get('researchFields',[]) + [f['name'][0:4] for f in ri[2]]\n",
    "    fc = Counter(researchFields)\n",
    "    fcl = [(a,fc[a]) for a in list(fc)]+[('0000',0)]\n",
    "    primaryFOR = sorted(fcl, key = lambda a: -a[1])[0][0]\n",
    "    \n",
    "    aff = (ri[0].get('affiliations',[{}])+[{}])[0]\n",
    "\n",
    "    return dict(\n",
    "              first_name = ri[0]['first_name'],\n",
    "              last_name = ri[0]['last_name'],\n",
    "              country = aff.get('country',''),\n",
    "              grid_id = aff.get('id',''),\n",
    "              insitution_name = aff.get('name',''),\n",
    "              size = researcher_size[ri[0]['researcher_id']],\n",
    "              year = ri[1],\n",
    "              primaryFOR = primaryFOR,\n",
    "              researchFields = researchFields\n",
    "            )\n",
    "    \n",
    "\n",
    "for ri in researcher_instance_list:\n",
    "    researcher = ri[0]['researcher_id']\n",
    "\n",
    "    if nodes.get(researcher,{'year':0})['year'] < ri[1]:       \n",
    "        nodes[researcher] = updateresearherdef(ri,nodes.get(researcher,{}))            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for k in nodes.keys():\n",
    "    del nodes[k]['researchFields']\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    G = nx.Graph()\n",
    "    G.add_nodes_from([(k,nodes[k]) \n",
    "                      for k in nodes.keys()])\n",
    "    G.add_edges_from([(e[0],e[1],{'weight':edges[e]}) for e in list(edges)])\n",
    "    #U = G.to_undirected()\n",
    "    #nx.set_node_attributes(U, 'Degree', nx.degree(U))\n",
    "    nx.write_graphml(G, \"eeCollaborations2.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### . You can now load the graphml file above into a network tool like Gephi to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.datetime.now()\n",
    "print(end)\n",
    "print('the process took ',end-start,' to complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
